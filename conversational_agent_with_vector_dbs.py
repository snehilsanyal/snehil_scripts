# -*- coding: utf-8 -*-
"""Conversational Agent with Vector DBs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t-dS1TH1q_NLT22iYjYHAz20JBlIZMI8

# Conversational Agents with Vector DBs using LangChain

Conversational agents can struggle with data freshness, knowledge about specific domains,or accessing internal documentation. By coupling agents with retrieval augmentation tools we no longer have these problems.

On the other side, using "naive" retrieval augmentation without the use of an agent means we will retrieve contexts with every query. Again, this isn't always ideal as not every query requires access to external knowledge.
Merging these methods gives us the best of both worlds.

## Requirements
Install libraries: tiktoken, datasets, pinecone-client[grpc], tensorflow-hub, tensorflow_text, tensorflow


## Pipeline
 1. Build knowledge base
 2. Initialize the Embedding Model
 3. Initialize the Vector DB
 4. Index the embeddings using VectorDB
 5. Create a Vector Store and Query
 6. Initialize Conversational Agent
 7. Using Conversational Agent

## Building our knowledge base
"""

#!pip install --quiet langchain tiktoken datasets pinecone-client[grpc] tensorflow_text sentence-transformers

from datasets import load_dataset 
# This dataset is already prepared: 
# Load SQUAD (Stanford Question Answering Dataset)
data = load_dataset('squad', split = 'train')
print(data)

# Thoughts: data format is arrow apache format which is faster
data = data.to_pandas()
print(data.head())

print(data.shape)

data.drop_duplicates(subset='context', keep='first', inplace=True)
print(data.shape)

"""## Initialize the Embedding Model and Vector DB

Thoughts: Embedding model: anything other than OpenAI's, are there open-source easy-to-use LLM embeddings? Without any OpenAI API problem?
Can we make our own embedding model? Semantic embedding Task? 

PS: Do google search: open source llm embeddings
"""

from langchain.embeddings import TensorflowHubEmbeddings, SentenceTransformerEmbeddings 
# Use Multilingual Sentence Encoder (16 languages), requires
# tensorflow_hub, tensorflow_text, tensorflow to be installed 
url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'
tf_embedding = TensorflowHubEmbeddings(model_url = url)
st_embedding = SentenceTransformerEmbeddings(model_name = 'all-MiniLM-L6-v2')

sentence = 'This is a sample sentence to compute embedding'
embed_1 = tf_embedding.embed_query(sentence)
embed_2 = st_embedding.embed_query(sentence)

print(len(embed_1), len(embed_2))

"""## Initialize the Vector Database (can use getpass)
 Thoughts: Available Vector DBs from LangChain like Pinecone

"""

# from getpass import getpass\
import pinecone
PINECONE_API_KEY = '6b71cd08-6594-4b48-ac6d-511db01262f4'
PINECONE_ENV = 'asia-southeast1-gcp-free'
index_name = 'langchain-retrieval-agent'
# Previously named index
pinecone.init(api_key = PINECONE_API_KEY, environment = PINECONE_ENV)

if index_name not in pinecone.list_indexes():
    # we create a new index
    pinecone.create_index(
        name=index_name,
        metric='dotproduct',
        dimension=384  # 512 dim of Universal Multilingual Sentence Encoder
    )

"""## Connect to the index

"""

index = pinecone.GRPCIndex(index_name)
print(index.describe_index_stats())

"""# Indexing
 Perform the indexing task using the LangChain vector store object. 
 It is much faster to do it via the Pinecone python client directly with batches. VectorDB provides a mechanism to optimize storage and querying of embeddings 
"""

from tqdm.auto import tqdm
from uuid import uuid4
batch_size = 100
texts = []
metadatas = []
for i in tqdm(range(0, len(data), batch_size)):
    # get end of batch
    i_end = min(len(data), i+batch_size)
    batch = data.iloc[i:i_end]
    # first get metadata fields for this record
    metadatas = [{
        'title': record['title'],
        'text': record['context']
    } for j, record in batch.iterrows()]
    # get the list of contexts / documents
    documents = batch['context']
    # create document embeddings
    # embeds = tf_embedding.embed_documents(documents)
    embeds = st_embedding.embed_documents(documents)
    # get IDs
    ids = batch['id']
    # add everything to pinecone
    index.upsert(vectors=zip(ids, embeds, metadatas))

print(index.describe_index_stats())

"""## Creating a Vector Store and Querying

Initialize a vector store using the same index.
"""

from langchain.vectorstores import Pinecone
text_field = "text"
embedding = st_embedding
# switch back to normal index for langchain
index = pinecone.Index(index_name)
vectorstore = Pinecone(
    index, embedding.embed_query, text_field
)

query = "when was the college of engineering in the University of Notre Dame established?"
vectorstore.similarity_search_with_score(
    query,  # our search query
    k=3  # return 3 most relevant docs
)

"""# Initializing the Conversational Agent

Conversational agent needs a Chat LLM, conversational memory, and a RetrievalQA chain to initialize.


"""
from transformers import OPTForCausalLM, AutoModelForCausalLM, AutoTokenizer, pipeline
# from langchain.chat_models import GooglePalmAI, Anthropic
from langchain.llms import HuggingFacePipeline
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.chains import RetrievalQA

from langchain import PromptTemplate, LLMChain
## chat completion llm

# Test with Galactica model
# Source: https://huggingface.co/facebook/galactica-125m
# Add device_map = 'auto' for GPU
path = '../IntellX-main/models/facebook_galactica-125m'
# model = OPTForCausalLM.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path)
tokenizer = AutoTokenizer.from_pretrained(path)
pipe = pipeline("text-generation", model=model, tokenizer = tokenizer,
      max_new_tokens=32, model_kwargs={"temperature":0})
hf_pipeline = HuggingFacePipeline(pipeline=pipe)
llm = hf_pipeline 
# conversational memory
conversational_memory = ConversationBufferWindowMemory(
    memory_key='chat_history',
    k=5,
    return_messages=True
)
# retrieval qa chain
qa = RetrievalQA.from_chain_type(
    llm= llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

print(qa.run(query))

# Initialize Tool
from langchain.agents import Tool
tools = [
    Tool(
        name='Knowledge Base',
        func=qa.run,
        description=(
            'use this tool when answering general knowledge queries to get '
            'more information about the topic'
        )
    )
]
from langchain.agents import initialize_agent

agent = initialize_agent(
    agent='chat-conversational-react-description',
    tools=tools,
    llm=llm,
    verbose=True,
    max_iterations=3,
    early_stopping_method='generate',
    memory=conversational_memory
)

## Using the Conversational Agent (Check this)
print(agent(query))
print(agent("what is 2 * 7?"))
print(agent("can you tell me some facts about the University of Notre Dame?"))
print(agent("can you summarize these facts in two short sentences"))
# Delete pinecone index finally, or rename 
pinecone.delete_index(index_name)